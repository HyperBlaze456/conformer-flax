{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conformer\n",
    "[Original paper](https://arxiv.org/pdf/2005.08100)\n",
    "\n",
    "This implementation is translated from PyTorch code, from lucidrains.\n",
    "\n",
    "[PyTorch Repo](https://github.com/lucidrains/conformer/blob/master/conformer/conformer.py)"
   ],
   "id": "da7ef0584d3cff0a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-11T14:58:57.036574Z",
     "start_time": "2025-06-11T14:58:57.031588Z"
    }
   },
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.flax import Rearrange"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define helper functions/classes\n",
   "id": "4a0d84413b3921bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T14:58:57.091047Z",
     "start_time": "2025-06-11T14:58:57.085725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def calc_same_padding(kernel_size: int) -> tuple[int, int]:\n",
    "    pad = kernel_size // 2\n",
    "    return pad, pad - (kernel_size + 1) % 2"
   ],
   "id": "9782f7a8158827c1",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T14:58:57.179410Z",
     "start_time": "2025-06-11T14:58:57.174483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# GLU, Swish doesn't have to be manually implemented.\n",
    "class DepthWiseConv1d(nnx.Module):\n",
    "    def __init__(self, chan_in, chan_out, kernel_size, padding, rngs: nnx.Rngs):\n",
    "        self.padding = padding\n",
    "        self.conv = nnx.Conv(chan_in, chan_out, kernel_size, rngs=rngs, feature_group_count = chan_in)\n",
    "    def __call__(self, x: jax.Array):\n",
    "\n",
    "        # x = jnp.pad(x, self.padding) NNX Conv가 자동으로 사용하는 'SAME' 옵션 때문에 패딩 필요 X\n",
    "        return self.conv(x)"
   ],
   "id": "fd05d987e3fd1027",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T14:58:57.279235Z",
     "start_time": "2025-06-11T14:58:57.273073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# No-cache-RoPE\n",
    "# We do not consider Key getting cached, neither the RoPE matrix calculation.\n",
    "def _rotate_half(x: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"Split \"\"\"\n",
    "    x1, x2 = x[..., ::2], x[..., 1::2] # Later check the performance of this\n",
    "    return rearrange(jnp.stack((-x2, x1), axis=-1), '... d r -> ... (d r)', r=2)\n",
    "\n",
    "def _apply_rope(q, k, sin, cos):\n",
    "    \"\"\"RoPE를 Q·K에 적용\"\"\"\n",
    "    q = (q * cos) + (_rotate_half(q) * sin)\n",
    "    k = (k * cos) + (_rotate_half(k) * sin)\n",
    "    return q, k\n",
    "\n",
    "class RotaryLookup(nnx.Module):\n",
    "    \"\"\"RoPE 사인·코사인 테이블을 만드는 보조 모듈\"\"\"\n",
    "    def __init__(self, dim: int, theta: int = 10000):\n",
    "        self.inv_freq = 1.0 / (theta ** (jnp.arange(0, dim, 2) / dim))\n",
    "\n",
    "    def __call__(self, seq_len: int, dtype=jnp.float32):\n",
    "        t = jnp.arange(seq_len, dtype=dtype)\n",
    "        freqs = jnp.outer(t, self.inv_freq)\n",
    "        sin, cos = jnp.sin(freqs), jnp.cos(freqs)\n",
    "        # (1 1 n d)로 브로드캐스트\n",
    "        return rearrange(sin, 'n d -> 1 1 n d'), rearrange(cos, 'n d -> 1 1 n d')"
   ],
   "id": "4833ff2fd490fa68",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T14:58:57.370260Z",
     "start_time": "2025-06-11T14:58:57.356579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Callable\n",
    "\n",
    "# attn, FFN, convolutional module to make Conformer\n",
    "class Scale(nnx.Module):\n",
    "    def __init__(self, scale, fn:Callable):\n",
    "        self.fn = fn\n",
    "        self.scale = scale\n",
    "    def __call__(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) * self.scale\n",
    "\n",
    "class PreNorm(nnx.Module):\n",
    "    def __init__(self, dim, fn:Callable, rng: nnx.Rngs):\n",
    "        self.norm = nnx.RMSNorm(dim, rngs=rng) # Let's not use LayerNorm, and improve to RMS\n",
    "        self.fn = fn\n",
    "    def __call__(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "class Attention(nnx.Module):\n",
    "    \"\"\"Now uses RoPE instead of lookup based positional embeddings\"\"\"\n",
    "    def __init__(self,\n",
    "                 dim: int,\n",
    "                 rngs: nnx.Rngs,\n",
    "                 heads: int = 8,\n",
    "                 dim_head: int = 64,\n",
    "                 dropout: float = 0.0):\n",
    "        self.heads = heads\n",
    "        self.dim_head = dim_head\n",
    "        inner = heads * dim_head\n",
    "\n",
    "        self.qkv = nnx.Linear(dim, inner * 3, use_bias=False, rngs=rngs)\n",
    "        self.out_proj = nnx.Linear(inner, dim, use_bias=False, rngs=rngs)\n",
    "\n",
    "        self.rope = RotaryLookup(dim_head)\n",
    "        self.dropout = nnx.Dropout(dropout, rngs=rngs)\n",
    "\n",
    "        # 스케일링 상수\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "    def __call__(self,\n",
    "                 x: jnp.ndarray,\n",
    "                 mask: jnp.ndarray | None = None,\n",
    "                 *,\n",
    "                 rngs: nnx.Rngs | None = None,\n",
    "                 deterministic: bool = True) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        x     : (batch, seq, dim)\n",
    "        mask  : (batch, seq) or (batch, seq, seq)\n",
    "        The original code did not use context nor context_mask, thus this is ignored\n",
    "        \"\"\"\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        # 1) QKV 생성 및 헤드 분할\n",
    "        qkv = self.qkv(x)                               # (b n 3*inner)\n",
    "        qkv = rearrange(qkv, 'b n (t h d) -> t b h n d',\n",
    "                        t=3, h=self.heads, d=self.dim_head)\n",
    "        q, k, v = qkv                                   # 각각 (b h n d)\n",
    "\n",
    "        # 2) RoPE 적용\n",
    "        sin, cos = self.rope(n, dtype=x.dtype)          # (1 1 n d)\n",
    "        q, k = _apply_rope(q, k, jnp.repeat(sin, repeats=2, axis=-1), jnp.repeat(cos, repeats=2, axis=-1))\n",
    "\n",
    "        # 3) 유사도 & 마스킹\n",
    "        attn_logits = jnp.einsum('b h i d, b h j d -> b h i j', q, k)\n",
    "        attn_logits *= self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            # (b 1 i j)로 브로드캐스트\n",
    "            mask = mask if mask.ndim == 3 else mask[:, None, :, None] & mask[:, None, None, :]\n",
    "            attn_logits = jnp.where(mask, attn_logits, jnp.finfo(x.dtype).min)\n",
    "\n",
    "        # 4) 어텐션 가중치\n",
    "        attn = jax.nn.softmax(attn_logits, axis=-1)                          # (b h i j)\n",
    "\n",
    "        # 5) 값 집계 후 출력 투사\n",
    "        out = jnp.einsum('b h i j, b h j d -> b h i d', attn, v)             # (b h n d)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.out_proj(out)\n",
    "        if not deterministic:\n",
    "            out = self.dropout(out, rngs=rngs)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        rngs: nnx.Rngs,\n",
    "        mult = 4,\n",
    "        dropout = 0.0\n",
    "    ):\n",
    "        self.net = nnx.Sequential(\n",
    "            nnx.Linear(dim, dim * mult, rngs = rngs),\n",
    "            nnx.swish,\n",
    "            nnx.Dropout(dropout, rngs=rngs),\n",
    "            nnx.Linear(dim * mult, dim, rngs = rngs),\n",
    "            nnx.Dropout(dropout, rngs=rngs)\n",
    "        )\n",
    "    def __call__(self, x: jax.Array):\n",
    "        return self.net(x)\n",
    "\n",
    "class ConformerConvModule(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        rngs: nnx.Rngs,\n",
    "        causal: bool = False,\n",
    "        expansion_factor = 2,\n",
    "        kernel_size = 31,\n",
    "        dropout = 0.\n",
    "    ):\n",
    "        inner_dim = dim * expansion_factor\n",
    "        padding = calc_same_padding(kernel_size) if not causal else (kernel_size - 1, 0)\n",
    "        # B, C, N으로 변환할 필요 없다.\n",
    "        self.net = nnx.Sequential(\n",
    "            nnx.RMSNorm(dim, rngs=rngs),\n",
    "            nnx.Conv(dim, inner_dim * 2, 1, rngs=rngs),\n",
    "            nnx.glu,\n",
    "            DepthWiseConv1d(inner_dim, inner_dim, kernel_size = kernel_size, padding = padding, rngs=rngs),\n",
    "            nnx.BatchNorm(inner_dim, rngs=rngs) if not causal else jax.nn.identity,\n",
    "            nnx.swish,\n",
    "            nnx.Conv(inner_dim, dim, 1, rngs=rngs),\n",
    "            nnx.Dropout(dropout, rngs=rngs)\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.net(x)"
   ],
   "id": "5c4a380d9c5694e6",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "실제 모델 만들기",
   "id": "a7274cf9e1edcc63"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T14:58:57.450790Z",
     "start_time": "2025-06-11T14:58:57.444163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ConformerBlock(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        rngs: nnx.Rngs,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        ff_mult = 4,\n",
    "        conv_expansion_factor = 2,\n",
    "        conv_kernel_size = 31,\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.,\n",
    "        conv_dropout = 0.,\n",
    "        conv_causal = False\n",
    "    ):\n",
    "        self.ff1 = FeedForward(dim = dim, mult = ff_mult, dropout = ff_dropout, rngs=rngs)\n",
    "        self.attn = Attention(dim = dim, dim_head = dim_head, heads = heads, dropout = attn_dropout, rngs=rngs)\n",
    "        self.conv = ConformerConvModule(dim = dim, causal = conv_causal, expansion_factor = conv_expansion_factor, kernel_size = conv_kernel_size, dropout = conv_dropout, rngs=rngs)\n",
    "        self.ff2 = FeedForward(dim = dim, mult = ff_mult, dropout = ff_dropout, rngs=rngs)\n",
    "\n",
    "        self.attn = PreNorm(dim, self.attn, rngs)\n",
    "        self.ff1 = Scale(0.5, PreNorm(dim, self.ff1, rngs))\n",
    "        self.ff2 = Scale(0.5, PreNorm(dim, self.ff2, rngs))\n",
    "\n",
    "        self.post_norm = nnx.RMSNorm(dim, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x, mask = None):\n",
    "        x = self.ff1(x) + x\n",
    "        x = self.attn(x, mask = mask) + x\n",
    "        x = self.conv(x) + x\n",
    "        x = self.ff2(x) + x\n",
    "        x = self.post_norm(x)\n",
    "        return x\n"
   ],
   "id": "cee72c970c2a8301",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T14:58:57.492883Z",
     "start_time": "2025-06-11T14:58:57.485912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Conformer(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        rngs: nnx.Rngs,\n",
    "        *,\n",
    "        depth,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        ff_mult = 4,\n",
    "        conv_expansion_factor = 2,\n",
    "        conv_kernel_size = 31,\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.,\n",
    "        conv_dropout = 0.,\n",
    "        conv_causal = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.layers = [ConformerBlock(dim=dim, rngs=rngs, dim_head=dim_head,heads=heads, ff_mult=ff_mult, conv_expansion_factor=conv_expansion_factor,conv_kernel_size=conv_kernel_size, attn_dropout=attn_dropout, ff_dropout=ff_dropout,conv_dropout=conv_dropout, conv_causal=conv_causal) for _ in range(depth)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ],
   "id": "c58b1e800c7ebd54",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T14:58:57.607438Z",
     "start_time": "2025-06-11T14:58:57.599915Z"
    }
   },
   "cell_type": "code",
   "source": "jax.devices()",
   "id": "b99409f216ef4a2e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CudaDevice(id=0)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T14:58:59.643266Z",
     "start_time": "2025-06-11T14:58:57.651622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "model_key = nnx.Rngs(42)\n",
    "batch_size = 2\n",
    "seq_len = 128\n",
    "dim = 256\n",
    "\n",
    "x = jax.random.normal(key, (batch_size, seq_len, dim))\n",
    "model = Conformer(dim=dim, depth=4, rngs=model_key)\n",
    "y = model(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", y.shape)"
   ],
   "id": "fc61962cae0db033",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 00:01:09.311631: E external/xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng3{k11=2} for conv %cudnn-conv.1 = (f32[2,512,128]{2,1,0}, u8[0]{0}) custom-call(%transpose, %bitcast.12), window={size=31 pad=15_15}, dim_labels=bf0_oi0->bf0, feature_group_count=512, custom_call_target=\"__cudnn$convForward\", metadata={op_name=\"jit(conv_general_dilated)/jit(main)/conv_general_dilated\" source_file=\"/usr/programming/uv/machine_learning/lib/python3.10/site-packages/flax/nnx/nn/linear.py\" source_line=800}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]} is taking a while...\n",
      "2025-06-11 23:58:58.541737: E external/xla/xla/service/slow_operation_alarm.cc:140] The operation took 581.677005ms\n",
      "Trying algorithm eng3{k11=2} for conv %cudnn-conv.1 = (f32[2,512,128]{2,1,0}, u8[0]{0}) custom-call(%transpose, %bitcast.12), window={size=31 pad=15_15}, dim_labels=bf0_oi0->bf0, feature_group_count=512, custom_call_target=\"__cudnn$convForward\", metadata={op_name=\"jit(conv_general_dilated)/jit(main)/conv_general_dilated\" source_file=\"/usr/programming/uv/machine_learning/lib/python3.10/site-packages/flax/nnx/nn/linear.py\" source_line=800}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]} is taking a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 128, 256)\n",
      "Output shape: (2, 128, 256)\n"
     ]
    }
   ],
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
